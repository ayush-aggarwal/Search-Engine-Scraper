from bs4 import BeautifulSoup as bs
import requests
import pymongo
from nltk.corpus import stopwords
from nltk.tokenize import wordpunct_tokenize
def ask(query,pages):
	stop_words = set(stopwords.words('english'))
	stop_words.update(['.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])
	client=pymongo.MongoClient()
	db=client.text_mining
	for j in range(1,int(pages)+1):
		data=requests.get("http://www.ask.com/web?q="+query+"&page="+str(j)).text
		data=data.split('<h2 class="web-result-title">')
		data=data[1:]
		for i in data:
			dic={}
			dic["search_engine"]="ask"
			dic["query"]=query
			soup=bs(i,"lxml")
			a=soup.find("a")
			dic["title"]=a.text
			dic["title_probablity"]=float(str(a.text).lower().split().count(query))/float(len(str(a.text).split()))
			dic["link"]=a.get("href")
			p=soup.find("p",{"class":"web-result-description"})
			list_of_words = [i.lower() for i in wordpunct_tokenize(str(p.text)) if i.lower() not in stop_words]
			dic["snippet"]=" ".join(list_of_words)
			dic["snippet_probablity"]=float(list_of_words.count(query))/float(len(list_of_words))
			#source=requests.get(dic["link"]).text
			#soup=bs(source,"lxml")
			#dic["source"]=soup.text
			db.search_results.insert(dic)
	
